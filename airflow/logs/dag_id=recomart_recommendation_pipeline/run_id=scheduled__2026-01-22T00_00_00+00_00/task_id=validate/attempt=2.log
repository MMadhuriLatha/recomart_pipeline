{"timestamp":"2026-01-22T05:14:01.143957Z","level":"critical","event":"\n******************************************* Received SIGSEGV *******************************************\nSIGSEGV (Segmentation Violation) signal indicates Segmentation Fault error which refers to\nan attempt by a program/library to write or read outside its allocated memory.\n\nIn Python environment usually this signal refers to libraries which use low level C API.\nMake sure that you use right libraries/Docker Images\nfor your architecture (Intel/ARM) and/or Operational System (Linux/macOS).\n\nSuggested way to debug\n======================\n  - Set environment variable 'PYTHONFAULTHANDLER' to 'true'.\n  - Start airflow services.\n  - Restart failed airflow task.\n  - Check 'scheduler' and 'worker' services logs for additional traceback\n    which might contain information about module/library where actual error happen.\n\nKnown Issues\n============\n\nNote: Only Linux-based distros supported as \"Production\" execution environment for Airflow.\n\nmacOS\n-----\n 1. Due to limitations in Apple's libraries not every process might 'fork' safe.\n    One of the general error is unable to query the macOS system configuration for network proxies.\n    If your are not using a proxy you could disable it by set environment variable 'no_proxy' to '*'.\n    See: https://github.com/python/cpython/issues/58037 and https://bugs.python.org/issue30385#msg293958\n********************************************************************************************************","logger":"task"}
{"timestamp":"2026-01-22T06:43:36.071646Z","level":"critical","event":"\n******************************************* Received SIGSEGV *******************************************\nSIGSEGV (Segmentation Violation) signal indicates Segmentation Fault error which refers to\nan attempt by a program/library to write or read outside its allocated memory.\n\nIn Python environment usually this signal refers to libraries which use low level C API.\nMake sure that you use right libraries/Docker Images\nfor your architecture (Intel/ARM) and/or Operational System (Linux/macOS).\n\nSuggested way to debug\n======================\n  - Set environment variable 'PYTHONFAULTHANDLER' to 'true'.\n  - Start airflow services.\n  - Restart failed airflow task.\n  - Check 'scheduler' and 'worker' services logs for additional traceback\n    which might contain information about module/library where actual error happen.\n\nKnown Issues\n============\n\nNote: Only Linux-based distros supported as \"Production\" execution environment for Airflow.\n\nmacOS\n-----\n 1. Due to limitations in Apple's libraries not every process might 'fork' safe.\n    One of the general error is unable to query the macOS system configuration for network proxies.\n    If your are not using a proxy you could disable it by set environment variable 'no_proxy' to '*'.\n    See: https://github.com/python/cpython/issues/58037 and https://bugs.python.org/issue30385#msg293958\n********************************************************************************************************","logger":"task"}
{"timestamp":"2026-01-22T07:08:01.108862Z","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2026-01-22T07:08:01.109253Z","level":"info","event":"Filling up the DagBag from /Users/apinto/Documents/BITS WILP/Sem2/Assignment/recomart_pipeline/airflow/dags/airflow_dag.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2026-01-22T07:08:02.841400Z","level":"warning","event":"The `airflow.operators.python.PythonOperator` attribute is deprecated. Please use `'airflow.providers.standard.operators.python.PythonOperator'`.","category":"DeprecatedImportWarning","filename":"/Users/apinto/Documents/BITS WILP/Sem2/Assignment/recomart_pipeline/airflow/dags/airflow_dag.py","lineno":2,"logger":"py.warnings"}
{"timestamp":"2026-01-22T07:08:02.854561Z","level":"info","event":"Task instance is in running state","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.854666Z","level":"info","event":" Previous state of the Task instance: TaskInstanceState.QUEUED","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.855041Z","level":"info","event":"Current task name:validate","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.855183Z","level":"info","event":"Dag name:recomart_recommendation_pipeline","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.855906Z","level":"info","event":"Starting data validation...","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.866960Z","level":"error","event":"2026-01-22 12:38:02 - data_validation - INFO - [logger.py:58] - Starting data_validation","logger":"task.stderr"}
{"timestamp":"2026-01-22T07:08:02.866954Z","level":"info","event":"Starting data_validation","logger":"data_validation","filename":"logger.py","lineno":58}
{"timestamp":"2026-01-22T07:08:02.867758Z","level":"error","event":"2026-01-22 12:38:02 - data_validation - INFO - [data_validator.py:105] - Validating user interaction data","logger":"task.stderr"}
{"timestamp":"2026-01-22T07:08:02.867773Z","level":"info","event":"Validating user interaction data","logger":"data_validation","filename":"data_validator.py","lineno":105}
{"timestamp":"2026-01-22T07:08:02.877723Z","level":"error","event":"2026-01-22 12:38:02 - data_validation - INFO - [data_validator.py:157] - Validation completed. Quality score: 100.0/100","logger":"task.stderr"}
{"timestamp":"2026-01-22T07:08:02.877721Z","level":"info","event":"Validation completed. Quality score: 100.0/100","logger":"data_validation","filename":"data_validator.py","lineno":157}
{"timestamp":"2026-01-22T07:08:02.878830Z","level":"error","event":"2026-01-22 12:38:02 - data_validation - INFO - [logger.py:65] - Completed data_validation in 0.01s","logger":"task.stderr"}
{"timestamp":"2026-01-22T07:08:02.878826Z","level":"info","event":"Completed data_validation in 0.01s","logger":"data_validation","filename":"logger.py","lineno":65}
{"timestamp":"2026-01-22T07:08:02.879346Z","level":"info","event":"","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879384Z","level":"info","event":"============================================================","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879412Z","level":"info","event":"DATA QUALITY REPORT","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879490Z","level":"info","event":"============================================================","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879534Z","level":"info","event":"Quality Score: 100.0/100","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879590Z","level":"info","event":"Total Rows: 9,910","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879614Z","level":"info","event":"Total Columns: 4","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879640Z","level":"info","event":"","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879662Z","level":"info","event":"Missing Values: 0","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879683Z","level":"info","event":"Duplicate Records: 0","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879703Z","level":"info","event":"","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879724Z","level":"info","event":"Report saved to: reports/data_quality_report.json","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.879745Z","level":"info","event":"============================================================","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.884720Z","level":"info","event":"Data quality score: 100.0/100","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.884976Z","level":"info","event":"Done. Returned value was: 100.0","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator","filename":"python.py","lineno":216}
{"timestamp":"2026-01-22T07:08:02.885073Z","level":"info","event":"Pushing xcom","ti":"RuntimeTaskInstance(id=UUID('019be483-78ed-719f-8044-fc6246c6d467'), task_id='validate', dag_id='recomart_recommendation_pipeline', run_id='scheduled__2026-01-22T00:00:00+00:00', try_number=2, dag_version_id=UUID('019be483-4b20-7641-8635-67f364380c0a'), map_index=-1, hostname='apinto-mbp', context_carrier={}, task=<Task(PythonOperator): validate>, bundle_instance=LocalDagBundle(name=dags-folder), max_tries=2, start_date=datetime.datetime(2026, 1, 22, 7, 8, 0, 480015, tzinfo=datetime.timezone.utc), end_date=None, state=<TaskInstanceState.RUNNING: 'running'>, is_mapped=False, rendered_map_index=None)","logger":"task","filename":"task_runner.py","lineno":1357}
{"timestamp":"2026-01-22T07:08:02.893634Z","level":"info","event":"Task instance in success state","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.893687Z","level":"info","event":" Previous state of the Task instance: TaskInstanceState.RUNNING","logger":"task.stdout"}
{"timestamp":"2026-01-22T07:08:02.894314Z","level":"info","event":"Task operator:<Task(PythonOperator): validate>","logger":"task.stdout"}
