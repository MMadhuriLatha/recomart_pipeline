{"timestamp":"2026-01-22T06:05:23.201701Z","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2026-01-22T06:05:23.202132Z","level":"info","event":"Filling up the DagBag from /Users/apinto/Documents/BITS WILP/Sem2/Assignment/recomart_pipeline/airflow/dags/airflow_dag.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2026-01-22T06:05:24.877553Z","level":"warning","event":"The `airflow.operators.python.PythonOperator` attribute is deprecated. Please use `'airflow.providers.standard.operators.python.PythonOperator'`.","category":"DeprecatedImportWarning","filename":"/Users/apinto/Documents/BITS WILP/Sem2/Assignment/recomart_pipeline/airflow/dags/airflow_dag.py","lineno":2,"logger":"py.warnings"}
{"timestamp":"2026-01-22T06:05:24.886998Z","level":"info","event":"Task instance is in running state","logger":"task.stdout"}
{"timestamp":"2026-01-22T06:05:24.887061Z","level":"info","event":" Previous state of the Task instance: TaskInstanceState.QUEUED","logger":"task.stdout"}
{"timestamp":"2026-01-22T06:05:24.887500Z","level":"info","event":"Current task name:train","logger":"task.stdout"}
{"timestamp":"2026-01-22T06:05:24.887530Z","level":"info","event":"Dag name:recomart_recommendation_pipeline","logger":"task.stdout"}
{"timestamp":"2026-01-22T06:05:24.888456Z","level":"info","event":"Starting model training...","logger":"task.stdout"}
{"timestamp":"2026-01-22T06:05:25.314056Z","level":"error","event":"2026-01-22 11:35:25 - model_training - INFO - [logger.py:58] - Starting model_training","logger":"task.stderr"}
{"timestamp":"2026-01-22T06:05:25.314047Z","level":"info","event":"Starting model_training","logger":"model_training","filename":"logger.py","lineno":58}
{"timestamp":"2026-01-22T06:05:25.314967Z","level":"error","event":"2026-01-22 11:35:25 - model_training - INFO - [recommender.py:247] - Starting training pipeline for collaborative model","logger":"task.stderr"}
{"timestamp":"2026-01-22T06:05:25.314966Z","level":"info","event":"Starting training pipeline for collaborative model","logger":"model_training","filename":"recommender.py","lineno":247}
{"timestamp":"2026-01-22T06:05:25.319930Z","level":"error","event":"2026-01-22 11:35:25 - model_training - INFO - [recommender.py:260] - Train size: 7928, Test size: 1982","logger":"task.stderr"}
{"timestamp":"2026-01-22T06:05:25.319924Z","level":"info","event":"Train size: 7928, Test size: 1982","logger":"model_training","filename":"recommender.py","lineno":260}
{"timestamp":"2026-01-22T06:05:25.481093Z","level":"error","event":"2026-01-22 11:35:25 - model_training - INFO - [recommender.py:56] - Starting model training","logger":"task.stderr"}
{"timestamp":"2026-01-22T06:05:25.481126Z","level":"info","event":"Starting model training","logger":"model_training","filename":"recommender.py","lineno":56}
{"timestamp":"2026-01-22T06:05:25.497842Z","level":"error","event":"2026-01-22 11:35:25 - model_training - INFO - [recommender.py:66] - Matrix shape: (1000, 500)","logger":"task.stderr"}
{"timestamp":"2026-01-22T06:05:25.497830Z","level":"info","event":"Matrix shape: (1000, 500)","logger":"model_training","filename":"recommender.py","lineno":66}
{"timestamp":"2026-01-22T06:05:25.498566Z","level":"error","event":"2026-01-22 11:35:25 - model_training - INFO - [recommender.py:74] - Performing SVD with 50 factors","logger":"task.stderr"}
{"timestamp":"2026-01-22T06:05:25.498568Z","level":"info","event":"Performing SVD with 50 factors","logger":"model_training","filename":"recommender.py","lineno":74}
{"timestamp":"2026-01-22T06:07:10.664785Z","level":"error","event":"Server indicated the task shouldn't be running anymore. Terminating process","detail":{"detail":{"reason":"not_found","message":"Task Instance not found"}},"logger":"task"}
{"timestamp":"2026-01-22T06:07:15.685779Z","level":"error","event":"Task killed!","logger":"task"}
{"timestamp":"2026-01-22T06:44:43.225806Z","level":"critical","event":"\n******************************************* Received SIGSEGV *******************************************\nSIGSEGV (Segmentation Violation) signal indicates Segmentation Fault error which refers to\nan attempt by a program/library to write or read outside its allocated memory.\n\nIn Python environment usually this signal refers to libraries which use low level C API.\nMake sure that you use right libraries/Docker Images\nfor your architecture (Intel/ARM) and/or Operational System (Linux/macOS).\n\nSuggested way to debug\n======================\n  - Set environment variable 'PYTHONFAULTHANDLER' to 'true'.\n  - Start airflow services.\n  - Restart failed airflow task.\n  - Check 'scheduler' and 'worker' services logs for additional traceback\n    which might contain information about module/library where actual error happen.\n\nKnown Issues\n============\n\nNote: Only Linux-based distros supported as \"Production\" execution environment for Airflow.\n\nmacOS\n-----\n 1. Due to limitations in Apple's libraries not every process might 'fork' safe.\n    One of the general error is unable to query the macOS system configuration for network proxies.\n    If your are not using a proxy you could disable it by set environment variable 'no_proxy' to '*'.\n    See: https://github.com/python/cpython/issues/58037 and https://bugs.python.org/issue30385#msg293958\n********************************************************************************************************","logger":"task"}
{"timestamp":"2026-01-22T14:50:02.445123Z","level":"critical","event":"\n******************************************* Received SIGSEGV *******************************************\nSIGSEGV (Segmentation Violation) signal indicates Segmentation Fault error which refers to\nan attempt by a program/library to write or read outside its allocated memory.\n\nIn Python environment usually this signal refers to libraries which use low level C API.\nMake sure that you use right libraries/Docker Images\nfor your architecture (Intel/ARM) and/or Operational System (Linux/macOS).\n\nSuggested way to debug\n======================\n  - Set environment variable 'PYTHONFAULTHANDLER' to 'true'.\n  - Start airflow services.\n  - Restart failed airflow task.\n  - Check 'scheduler' and 'worker' services logs for additional traceback\n    which might contain information about module/library where actual error happen.\n\nKnown Issues\n============\n\nNote: Only Linux-based distros supported as \"Production\" execution environment for Airflow.\n\nmacOS\n-----\n 1. Due to limitations in Apple's libraries not every process might 'fork' safe.\n    One of the general error is unable to query the macOS system configuration for network proxies.\n    If your are not using a proxy you could disable it by set environment variable 'no_proxy' to '*'.\n    See: https://github.com/python/cpython/issues/58037 and https://bugs.python.org/issue30385#msg293958\n********************************************************************************************************","logger":"task"}
